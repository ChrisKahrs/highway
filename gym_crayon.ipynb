{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Switch to hw4_env environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blackjack\n",
    "- Self Play\n",
    "- Reward Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym.utils.play import play, PlayPlot\n",
    "\n",
    "def callback(obs_t, obs_tp1, action, rew, terminated, truncated, info):\n",
    "       return [rew,]\n",
    "   \n",
    "plotter = PlayPlot(callback, 150, [\"reward\"])\n",
    "kta = {(100,):0,(119,):1}\n",
    "# Unsure why KTA is not working\n",
    "\n",
    "play(gym.make(\"Blackjack-v1\", render_mode=\"rgb_array\"), callback=plotter.callback, \n",
    "              keys_to_action=kta\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blackjack \n",
    "- human input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym.utils.play import play, PlayPlot\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\", render_mode=\"human\")\n",
    "\n",
    "obs, info = env.reset(seed=2)\n",
    "env.render()\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    action = int(input(\"Enter action: \"))\n",
    "    obs, rew, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    print(\"obs:\", obs)\n",
    "    print(\"Reward: \", rew)\n",
    "    print(\"Terminated: \", terminated)\n",
    "    print(\"truncated: \", truncated)\n",
    "    print(\"info: \", info)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial: https://gymnasium.farama.org/tutorials/blackjack_tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Car Racing\n",
    "* human play\n",
    "* Reward plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym.utils.play import play, PlayPlot\n",
    "\n",
    "def callback(obs_t, obs_tp1, action, rew, terminated, truncated, info):\n",
    "       return [rew,]\n",
    "   \n",
    "plotter = PlayPlot(callback, 150, [\"reward\"])\n",
    "\n",
    "play(gym.make(\"CarRacing-v2\", render_mode=\"rgb_array\"), callback=plotter.callback, \n",
    "              keys_to_action={\n",
    "                                               \"w\": np.array([0, 0.7, 0]),\n",
    "                                               \"a\": np.array([-1, 0, 0]),\n",
    "                                               \"s\": np.array([0, 0, 1]),\n",
    "                                               \"d\": np.array([1, 0, 0]),\n",
    "                                               \"wa\": np.array([-1, 0.7, 0]),\n",
    "                                               \"dw\": np.array([1, 0.7, 0]),\n",
    "                                               \"ds\": np.array([1, 0, 1]),\n",
    "                                               \"as\": np.array([-1, 0, 1]),\n",
    "                                              }, noop=np.array([0,0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better human racing: python gymnasium/envs/box2d/car_racing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar Lander\n",
    "- unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.utils.play import play\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode='rgb_array', gravity=-5.0)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "kta = {(100,):1,(119,):2, (97,):3}\n",
    "play(env, zoom=2, keys_to_action=kta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highway Gym\n",
    "* manual play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ckahrs\\Anaconda3\\envs\\hw4_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:32: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001b[0m\n",
      "  \"A Box observation space has an unconventional shape (neither an image, nor a 1D vector). \"\n",
      "c:\\Users\\ckahrs\\Anaconda3\\envs\\hw4_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:175: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  \"Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\"\n",
      "c:\\Users\\ckahrs\\Anaconda3\\envs\\hw4_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:188: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  \"Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\"\n",
      "c:\\Users\\ckahrs\\Anaconda3\\envs\\hw4_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:196: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  f\"The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `{type(result)}`\"\n",
      "c:\\Users\\ckahrs\\Anaconda3\\envs\\hw4_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:220: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  \"Core environment is written in old step API which returns one bool instead of two. \"\n",
      "c:\\Users\\ckahrs\\Anaconda3\\envs\\hw4_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:290: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info:  {'speed': 25.0, 'crashed': False, 'action': 1, 'cost': 0.0}\n",
      "reward:  0.8666666666666667\n",
      "count:  0\n",
      "info:  {'speed': 25.0, 'crashed': False, 'action': 1, 'cost': 0.0}\n",
      "reward:  0.8666666666666667\n",
      "count:  1\n",
      "info:  {'speed': 29.1455588268693, 'crashed': False, 'action': 2, 'cost': 0.0}\n",
      "reward:  0.9772149020498481\n",
      "count:  2\n",
      "info:  {'speed': 29.853986056331806, 'crashed': False, 'action': 3, 'cost': 0.0}\n",
      "reward:  0.9961062948355148\n",
      "count:  3\n",
      "info:  {'speed': 29.975047934935745, 'crashed': False, 'action': 4, 'cost': 0.0}\n",
      "reward:  0.9993346115982865\n",
      "count:  4\n",
      "info:  {'speed': 29.995735985650892, 'crashed': False, 'action': 1, 'cost': 0.0}\n",
      "reward:  0.9998862929506904\n",
      "count:  5\n",
      "info:  {'speed': 29.99927133011546, 'crashed': False, 'action': 0, 'cost': 0.0}\n",
      "reward:  0.9999805688030788\n",
      "count:  6\n",
      "info:  {'speed': 29.999875478889802, 'crashed': False, 'action': 1, 'cost': 0.0}\n",
      "reward:  0.96926673491117\n",
      "count:  7\n",
      "info:  {'speed': 29.999978720807306, 'crashed': False, 'action': 1, 'cost': 0.0}\n",
      "reward:  0.9776548432680897\n",
      "count:  8\n",
      "info:  {'speed': 29.999996363636328, 'crashed': False, 'action': 3, 'cost': 0.0}\n",
      "reward:  0.9777772948339022\n",
      "count:  9\n",
      "info:  {'speed': 29.99999937858823, 'crashed': False, 'action': 2, 'cost': 0.0}\n",
      "reward:  0.9526461658871632\n",
      "count:  10\n",
      "info:  {'speed': 29.999999893808038, 'crashed': False, 'action': 4, 'cost': 0.0}\n",
      "reward:  0.9555357836597115\n",
      "count:  11\n",
      "info:  {'speed': 29.99999998185304, 'crashed': False, 'action': 4, 'cost': 0.0}\n",
      "reward:  0.955555507282971\n",
      "count:  12\n",
      "info:  {'speed': 29.9999999968989, 'crashed': False, 'action': 3, 'cost': 0.0}\n",
      "reward:  0.9692714845974124\n",
      "count:  13\n",
      "info:  {'speed': 29.99999999947006, 'crashed': False, 'action': 2, 'cost': 0.0}\n",
      "reward:  0.9776554128823084\n",
      "count:  14\n",
      "info:  {'speed': 29.99999999990944, 'crashed': False, 'action': 2, 'cost': 0.0}\n",
      "reward:  0.9777773917903229\n",
      "count:  15\n",
      "info:  {'speed': 29.999999999984524, 'crashed': False, 'action': 2, 'cost': 0.0}\n",
      "reward:  0.9914878522348297\n",
      "count:  16\n",
      "info:  {'speed': 29.999999999997357, 'crashed': False, 'action': 4, 'cost': 0.0}\n",
      "reward:  0.9998776197333902\n",
      "count:  17\n",
      "info:  {'speed': 29.99999999999955, 'crashed': False, 'action': 4, 'cost': 0.0}\n",
      "reward:  0.9999996140262714\n",
      "count:  18\n",
      "info:  {'speed': 29.999999999999922, 'crashed': False, 'action': 4, 'cost': 0.0}\n",
      "reward:  0.9718182549563693\n",
      "count:  19\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 0, 'cost': 0.0}\n",
      "reward:  0.9777179501690257\n",
      "count:  20\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 2, 'cost': 0.0}\n",
      "reward:  0.9507107511075491\n",
      "count:  21\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 2, 'cost': 0.0}\n",
      "reward:  0.9555138197491172\n",
      "count:  22\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 0, 'cost': 0.0}\n",
      "reward:  0.9555554441205868\n",
      "count:  23\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 0, 'cost': 0.0}\n",
      "reward:  0.9755669868772644\n",
      "count:  24\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 1, 'cost': 0.0}\n",
      "reward:  0.9777642307506286\n",
      "count:  25\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 2, 'cost': 0.0}\n",
      "reward:  0.9777777465946883\n",
      "count:  26\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 2, 'cost': 0.0}\n",
      "reward:  0.9777777777437247\n",
      "count:  27\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 4, 'cost': 0.0}\n",
      "reward:  0.997093225210067\n",
      "count:  28\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 3, 'cost': 0.0}\n",
      "reward:  0.9999802359546176\n",
      "count:  29\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 2, 'cost': 0.0}\n",
      "reward:  0.9999999522094928\n",
      "count:  30\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 0, 'cost': 0.0}\n",
      "reward:  0.9999999999448038\n",
      "count:  31\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 3, 'cost': 0.0}\n",
      "reward:  0.9765434959512097\n",
      "count:  32\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 3, 'cost': 0.0}\n",
      "reward:  0.9777714831245614\n",
      "count:  33\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 3, 'cost': 0.0}\n",
      "reward:  0.9983477217843216\n",
      "count:  34\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 4, 'cost': 0.0}\n",
      "reward:  0.9999907690977007\n",
      "count:  35\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 3, 'cost': 0.0}\n",
      "reward:  0.9999999797296034\n",
      "count:  36\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 1, 'cost': 0.0}\n",
      "reward:  0.9999999999790786\n",
      "count:  37\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 4, 'cost': 0.0}\n",
      "reward:  0.9999999999999956\n",
      "count:  38\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 4, 'cost': 0.0}\n",
      "reward:  0.9999999999999997\n",
      "count:  39\n",
      "info:  {'speed': 29.999999999999986, 'crashed': False, 'action': 4, 'cost': 0.0}\n",
      "Total reward: 39.080291533732705\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import highway_env\n",
    "\n",
    "config = {\n",
    "    \"simulation_frequency\": 15,\n",
    "    \"show_trajectories\": False,\n",
    "    \"manual_control\": True,\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"vehicles_count\": 15,\n",
    "        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\", \"cos_h\", \"sin_h\"],\n",
    "        \"features_range\": {\n",
    "            \"x\": [-100, 100],\n",
    "            \"y\": [-100, 100],\n",
    "            \"vx\": [-20, 20],\n",
    "            \"vy\": [-20, 20]\n",
    "        },\n",
    "        \"absolute\": False,\n",
    "        \"order\": \"sorted\"\n",
    "    }\n",
    "}\n",
    "# env = gym.make('highway-fast-v0')\n",
    "env = gym.make('highway-v0')\n",
    "env.configure(config)\n",
    "# env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "\n",
    "total_reward = 0\n",
    "count = 0\n",
    "env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "    print(\"info: \", info)\n",
    "    print(\"reward: \", reward)\n",
    "    print(\"count: \", count)\n",
    "    total_reward += reward\n",
    "    count += 1\n",
    "    env.render()\n",
    "    \n",
    "print(\"info: \", info)\n",
    "print(\"Total reward:\", total_reward)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('hw4_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6241628ca03169c539073f9270cd151db60cb14e39652deee86987017b204276"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
